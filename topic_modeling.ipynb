{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/blehman/anaconda/lib/python2.7/site-packages/rpy2/robjects/robject.py:5: UserWarning: Loading required package: stats\n",
      "\n",
      "  rpy2.rinterface.initr()\n",
      "/Users/blehman/anaconda/lib/python2.7/site-packages/rpy2/robjects/robject.py:5: UserWarning: Loading required package: DBI\n",
      "\n",
      "  rpy2.rinterface.initr()\n",
      "/Users/blehman/anaconda/lib/python2.7/site-packages/rpy2/robjects/robject.py:5: UserWarning: Loading required package: rJava\n",
      "\n",
      "  rpy2.rinterface.initr()\n",
      "/Users/blehman/anaconda/lib/python2.7/site-packages/rpy2/robjects/robject.py:5: UserWarning: Error in .jcall(drv@jdrv, \"Ljava/sql/Connection;\", \"connect\", as.character(url)[1],  : \n",
      "  java.sql.SQLException: [Vertica][VJDBC](100176) Failed to connect to host localhost on port 5434. Reason: Connection refused\n",
      "\n",
      "  rpy2.rinterface.initr()\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import itertools\n",
    "import pickle\n",
    "import hickle \n",
    "import gzip\n",
    "import operator\n",
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "import pprint as pp\n",
    "import collections\n",
    "import ConfigParser\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import twitter\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "# bokeh\n",
    "import bokeh.plotting as bkplt\n",
    "\n",
    "\n",
    "# import requirments \n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import rpy2\n",
    "%load_ext rpy2.ipython\n",
    "%R require(\"ggplot2\")\n",
    "% matplotlib inline\n",
    "from ggplot import *\n",
    "randn = np.random.randn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "Let's grab some json records from Twitter's public api. \n",
    "\n",
    "We'll use python-twitter. \n",
    "<pre>\n",
    "$ pip install python-twitter\n",
    "$ pydoc twitter.Api\n",
    "</pre>\n",
    "\n",
    "Build an app [https://apps.twitter.com/](https://apps.twitter.com/).  \n",
    "\n",
    "Then use the app info in the `config.cfg` file.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrianLehman\n"
     ]
    }
   ],
   "source": [
    "# read the config file.\n",
    "config = ConfigParser.RawConfigParser()\n",
    "config.read('myconfig.cfg')\n",
    "\n",
    "# provided w/ Twitter app. See https://apps.twitter.com/\n",
    "token = config.get('oauth','token')\n",
    "token_secret = config.get('oauth','token_secret')\n",
    "con_key = config.get('oauth','con_key')\n",
    "con_secret_key = config.get('oauth','con_secret_key')\n",
    "\n",
    "# setup \n",
    "api = twitter.Api(\n",
    "    consumer_key=con_key\n",
    "    , consumer_secret=con_secret_key\n",
    "    , access_token_key = token\n",
    "    , access_token_secret = token_secret)\n",
    "\n",
    "# test creds\n",
    "print \"@{}\".format(api.VerifyCredentials().GetScreenName())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run query\n",
    "results = api.GetSearch(term = 'golden retriever'\n",
    "                        , count = 100\n",
    "                        , include_entities=True)\n",
    "counter = 0\n",
    "total_tweets = 5000\n",
    "tweets = []\n",
    "while counter <= total_tweets:\n",
    "    if counter == 1:\n",
    "        new_results = api.GetSearch(term = 'golden retriever'\n",
    "                                    , count = 100\n",
    "                                    , max_id = results[-1].GetId()\n",
    "                                    , include_entities=True)\n",
    "    else:\n",
    "        new_results = api.GetSearch(term = 'golden retriever'\n",
    "                                    , count = 100\n",
    "                                    , max_id = new_results[-1].GetId()\n",
    "                                    , include_entities=True)\n",
    "    counter += len(new_results)\n",
    "    tweets.extend(new_results)\n",
    "#for i,item in enumerate(tweets):\n",
    "#    print i,' ',item.GetText()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5081"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text = [tweet.GetText() for tweet in tweets]\n",
    "len(tweet_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Topic Model\n",
    "Three steps:  \n",
    "1.  Set up a training and test set.\n",
    "2.  Set up a vecterizer.\n",
    "3.  Build the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_index(total_tweets):\n",
    "        \"\"\"\n",
    "        Builds an index for the training and test set.\n",
    "        The sets serve as a list of row numbers to extract from the dataset. \n",
    "        \"\"\"\n",
    "        # based on the total tweet count, create an array of all line numbers \n",
    "        line_index = np.array(range(0,total_tweets))\n",
    "        # split the array into training and test sets of index values\n",
    "        trainIndex,testIndex = train_test_split(line_index,train_size=0.70, random_state=42)\n",
    "        # save test & traning index values\n",
    "        #np.save(\"training_index\",trainIndex)\n",
    "        #np.save(\"testing_index\",testIndex)\n",
    "        return trainIndex,testIndex\n",
    "\n",
    "trainIndex,testIndex = create_index(len(tweet_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# see http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "def vectorize_1(vocab=[]):\n",
    "    vectorizer = TfidfVectorizer(#min_df=20\n",
    "                                 stop_words='english'\n",
    "                                 , use_idf=True # enable inverse-document-frequency reweighting\n",
    "                                 , ngram_range=(1,2) # given our vocab, not really necessary\n",
    "                                 , binary = True # presence of word instead of frequency\n",
    "                                 #, vocabulary = vocab\n",
    "                                ) \n",
    "    #X = vectorizer.fit_transform(tweet_list)\n",
    "    return vectorizer\n",
    "\n",
    "def vectorize_2():\n",
    "    vectorizer = CountVectorizer(min_df=20\n",
    "                                 , stop_words='english'\n",
    "                                 , ngram_range=(1,2) # given our vocab, not really necessary\n",
    "                                 , binary = True # presence of word instead of frequency\n",
    "                                 #, vocabulary = set(vocab)\n",
    "                                ) \n",
    "    #X = vectorizer.fit_transform(tweet_list)\n",
    "    return vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = vectorize_1()\n",
    "X = vectorizer.fit_transform([tweet_text[i] for i in trainIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#explained_variances = np.var(X_svd, axis=0) / np.var(X_train, axis=0).sum()\n",
    "\n",
    "### \n",
    "### This can take 10-15 mins\n",
    "###\n",
    "def create_svd_doc_term_matrix(X_train, num_eigen_vectors=100):\n",
    "    \"\"\"\n",
    "    Create the array with truncated svd.\n",
    "    \"\"\"\n",
    "    svd = TruncatedSVD(n_components = num_eigen_vectors)\n",
    "    lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "    return lsa.fit_transform(X_train), svd\n",
    "\n",
    "run_here = False\n",
    "if run_here:\n",
    "    explained_variance_red = []\n",
    "    explained_variance_nouns = []\n",
    "    svd_component_range = range(100,351,50)\n",
    "    for i in svd_component_range:\n",
    "        # find explained variance in manual reduction method\n",
    "        X_svd_red, svd_red = create_svd_doc_term_matrix(X_reduced,i)\n",
    "        explained_variance_red.append(svd_red.explained_variance_ratio_.sum())\n",
    "        # find explained variance in noun reduction method\n",
    "        X_svd_noun, svd_noun = create_svd_doc_term_matrix(X_noun,i*3)\n",
    "        explained_variance_nouns.append(svd_noun.explained_variance_ratio_.sum())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
