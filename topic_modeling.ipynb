{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "import pickle\n",
    "import hickle \n",
    "import gzip\n",
    "import operator\n",
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "import pprint as pp\n",
    "import collections\n",
    "import ConfigParser\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import twitter\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "# bokeh\n",
    "import bokeh.plotting as bkplt\n",
    "\n",
    "\n",
    "# import requirments \n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import rpy2\n",
    "%load_ext rpy2.ipython\n",
    "%R require(\"ggplot2\")\n",
    "% matplotlib inline\n",
    "from ggplot import *\n",
    "randn = np.random.randn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "Let's grab some json records from Twitter's public api. \n",
    "\n",
    "We'll use python-twitter. \n",
    "<pre>\n",
    "$ pip install python-twitter\n",
    "$ pydoc twitter.Api\n",
    "</pre>\n",
    "\n",
    "Build an app [https://apps.twitter.com/](https://apps.twitter.com/).  \n",
    "\n",
    "Then use the app info in the `config.cfg` file.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the config file.\n",
    "config = ConfigParser.RawConfigParser()\n",
    "config.read('myconfig.cfg')\n",
    "\n",
    "# provided w/ Twitter app. See https://apps.twitter.com/\n",
    "token = config.get('oauth','token')\n",
    "token_secret = config.get('oauth','token_secret')\n",
    "con_key = config.get('oauth','con_key')\n",
    "con_secret_key = config.get('oauth','con_secret_key')\n",
    "\n",
    "# setup \n",
    "api = twitter.Api(\n",
    "    consumer_key=con_key\n",
    "    , consumer_secret=con_secret_key\n",
    "    , access_token_key = token\n",
    "    , access_token_secret = token_secret)\n",
    "\n",
    "# test creds\n",
    "print \"@{}\".format(api.VerifyCredentials().GetScreenName())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = api.GetSearch(term = 'golden retriever', count = 100, include_entities=True)                \n",
    "counter = 1\n",
    "total_tweets = 5000\n",
    "tweets = []\n",
    "while counter <= total_tweets:\n",
    "    if counter == 1:\n",
    "        new_results = api.GetSearch(term = 'golden retriever'\n",
    "                                    , count = 100\n",
    "                                    , max_id = results[-1].GetId()\n",
    "                                    , include_entities=True)\n",
    "    else:\n",
    "        new_results = api.GetSearch(term = 'golden retriever'\n",
    "                                    , count = 100\n",
    "                                    , max_id = new_results[-1].GetId()\n",
    "                                    , include_entities=True)\n",
    "    counter += len(new_results)\n",
    "    tweets.extend(new_results)\n",
    "#for i,item in enumerate(tweets):\n",
    "#    print i,' ',item.GetText()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_text = [tweet.GetText() for tweet in tweets]\n",
    "print len(tweet_text)\n",
    "pickle.dump(tweet_text,open('./data/tweet_text.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test set\n",
    "Split the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up a training and test set.\n",
    "\n",
    "def create_index(total_tweets):\n",
    "        \"\"\"\n",
    "        Builds an index for the training and test set.\n",
    "        The sets serve as a list of row numbers to extract from the dataset. \n",
    "        \"\"\"\n",
    "        # based on the total tweet count, create an array of all line numbers \n",
    "        line_index = np.array(range(0,total_tweets))\n",
    "        # split the array into training and test sets of index values\n",
    "        trainIndex,testIndex = train_test_split(line_index,train_size=0.70, random_state=42)\n",
    "        # save test & traning index values\n",
    "        #np.save(\"training_index\",trainIndex)\n",
    "        #np.save(\"testing_index\",testIndex)\n",
    "        return trainIndex,testIndex\n",
    "\n",
    "# build indicies \n",
    "trainIndex,testIndex = create_index(len(tweet_text))\n",
    "pickle.dump(trainIndex,open('data/trainIndex.pkl','wb'))\n",
    "pickle.dump(testIndex,open('data/testIndex.pkl','wb'))\n",
    "\n",
    "# build test set\n",
    "test_tweets = [tweet_text[i] for i in testIndex]\n",
    "pickle.dump(test_tweets,open('data/test_tweets.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the Tweets\n",
    "Three steps:  \n",
    "1.  Set up a vecterizer.\n",
    "2.  Vectorize the tweets to build the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up a vecterizer.\n",
    "# see http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "def vectorize_1():\n",
    "    vectorizer = TfidfVectorizer(#min_df=20\n",
    "                                 stop_words='english'\n",
    "                                 , use_idf=True # enable inverse-document-frequency reweighting\n",
    "                                 , ngram_range=(1,2) # given our vocab, not really necessary\n",
    "                                 , binary = True # presence of word instead of frequency\n",
    "                                 #, vocabulary = vocab\n",
    "                                ) \n",
    "    #X = vectorizer.fit_transform(tweet_list)\n",
    "    return vectorizer\n",
    "\n",
    "def vectorize_2(vocab):\n",
    "    vectorizer = CountVectorizer(stop_words='english'\n",
    "                                 , ngram_range=(1,2) # given our vocab, not really necessary\n",
    "                                 , binary = True # presence of word instead of frequency\n",
    "                                 , vocabulary = set(vocab)\n",
    "                                ) \n",
    "    #X = vectorizer.fit_transform(tweet_list)\n",
    "    return vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vectorize the tweets to build the vocabulary.\n",
    "vectorizer = vectorize_1()\n",
    "X = vectorizer.fit_transform([tweet_text[i] for i in trainIndex])\n",
    "X.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Progress: \n",
    "* Total tweets: 3,556. \n",
    "\n",
    "* Total tokens: 12,573. \n",
    "\n",
    "### Row Reduction\n",
    "\n",
    "To choose the appropriate number of svd components, we need to explore the amount of variance explained with each component. We'll reduce the number of components to 600. This number provides about 90% of the explained variance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#explained_variances = np.var(X_svd, axis=0) / np.var(X_train, axis=0).sum()\n",
    "\n",
    "### \n",
    "### This can take 10-15 mins\n",
    "###\n",
    "def create_svd_doc_term_matrix(X_train, num_eigen_vectors=100):\n",
    "    \"\"\"\n",
    "    Create the array with truncated svd.\n",
    "    \"\"\"\n",
    "    svd = TruncatedSVD(n_components = num_eigen_vectors)\n",
    "    pipeline = make_pipeline(svd, Normalizer(copy=False))\n",
    "    return pipeline.fit_transform(X_train), svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "explained_variance_list = []\n",
    "svd_component_range = range(100,751,50)\n",
    "for i in svd_component_range:\n",
    "    # find explained variance in manual reduction method\n",
    "    X_svd, svd = create_svd_doc_term_matrix(X,i)\n",
    "    explained_variance_list.append(svd.explained_variance_ratio_.sum())\n",
    "\n",
    "expVar = pd.DataFrame({'explained_var':explained_variance_list\n",
    "                   , 'components':svd_component_range})\n",
    "display(expVar)\n",
    "#%Rpush  expVar\n",
    "display(expVar.plot(x='components',y='explained_var'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Progress:\n",
    "* Total tweets: 3,556. \n",
    "\n",
    "* Total <s>tokens</s> components: <s>10,363</s> 600.   \n",
    "\n",
    "### Create Cluster Centroids\n",
    "We'll now apply kmeans to find the centroids that will be used to predict a cluster for each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_clusters(X_svd, k=5):\n",
    "    \"\"\"\n",
    "    Use kmeans to find centroids.\n",
    "    \"\"\"\n",
    "    km = KMeans(n_clusters=k\n",
    "                , init='k-means++'\n",
    "                , max_iter=100\n",
    "                #, n_init=10\n",
    "                , verbose=False)\n",
    "    km.fit(X_svd)\n",
    "    pred=km.predict(X_svd)\n",
    "    pred_df=pd.DataFrame(pred)\n",
    "    pred_df.columns=['pred_cluster']\n",
    "    return km.cluster_centers_ , pred_df, k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose number of clusters\n",
    "my_k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build centroids\n",
    "centroids, predictions, n_clusters = build_clusters(X_svd, my_k)\n",
    "pickle.dump(centroids, open('./data/centroids.pkl','wb'))\n",
    "pickle.dump(predictions, open('./data/predictions.pkl','wb')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Word Loadings\n",
    "Those tweets nearest the cluster centers are used as an approximation for their meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_loadings = np.dot(centroids, svd.components_)\n",
    "pickle.dump(word_loadings,open('./data/word_loadings.pkl','wb'))\n",
    "vocab = vectorizer.get_feature_names()\n",
    "pickle.dump(vocab,open('./data/vocab.pkl','wb'))\n",
    "for k in range(0,my_k):\n",
    "    #word loadings = cluster_centers * eigenvectors \n",
    "    indices=[i for i in np.argsort(word_loadings[k,:])[::-1]]    \n",
    "    sorted_vocab=[vocab[i] for i in indices]\n",
    "    print(\"Top words for cluster {}:\\n{}\\n\".format(k, sorted_vocab[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label New Tweets\n",
    "Apply the model to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check = pickle.load(open('./data/vocab.pkl','rb'))\n",
    "check[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_tweets(vectorizer, word_loadings, testing_data, sample_percentage=0.20):\n",
    "    \"\"\"\n",
    "    Label tweets.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    sample_size = int(len(testing_data)*sample_percentage)\n",
    "    sample_tweets = testing_data[:sample_size]\n",
    "    for tweet in sample_tweets:\n",
    "        # vectorize the tweet\n",
    "        sparse_array = vectorizer.fit_transform([tweet])\n",
    "        # subtract all values between the tweet vectorization and centroids\n",
    "        sparse_array_subtraction_abs = np.absolute(sparse_array - word_loadings)\n",
    "        # sum to get the total distances \n",
    "        sparse_array_subtraction_abs_sum = sparse_array_subtraction_abs.sum(axis=1)\n",
    "        # append the index of the minimum distance\n",
    "        result.append(np.argmin(sparse_array_subtraction_abs_sum))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_vectorizer = vectorize_2(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TopicModel():\n",
    "    \"\"\"\n",
    "    Topic clusters built from 2012 soda rules, SVD applied, kmeans for centroids. New tweets\n",
    "    are labled by their nearness to centroids.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Load and initialize any external models or data here.\n",
    "        \"\"\"\n",
    "        self.word_loadings = pickle.load(open('./data/word_loadings.pkl'))\n",
    "        self.vocab = pickle.load(open('./data/vocab.pkl'))\n",
    "        self.vectorizer = CountVectorizer(stop_words='english'\n",
    "                                          , ngram_range = (1,2)\n",
    "                                          , binary = True # presence of word instead of frequency\n",
    "                                          , vocabulary = self.vocab\n",
    "                                         )\n",
    "        self.tweets = pickle.load(open('./data/test_tweets.pkl')) \n",
    "    def enrichment_value(self):\n",
    "        \"\"\"\n",
    "        Calculates the nearest cluster for an unlabeled tweet using the vocab and cluster centers from the training set.\n",
    "        \"\"\"\n",
    "        tweetTxt = self.tweets\n",
    "        # vectorize the tweet\n",
    "        sparse_array = self.vectorizer.fit_transform(tweetTxt).toarray()\n",
    "        for item in sparse_array:\n",
    "            # multiply the weights by the vector\n",
    "            weighted_sparse_array = item * self.word_loadings\n",
    "            # dot product to find the sum of the token weights for this specific tweet\n",
    "            sums = [np.dot(item,row) for row in self.word_loadings]\n",
    "            # return the following\n",
    "            result = {\n",
    "                    \"clusterID\": np.argmax(sums)\n",
    "                    , \"min_score\": np.min(sums)\n",
    "                    , \"max_score\": np.max(sums)\n",
    "                    , \"mean_score\": np.mean(sums)\n",
    "                    , \"stdev_score\": np.std(sums)\n",
    "                    }\n",
    "            yield result\n",
    "    def __repr__(self):\n",
    "        \"\"\" Add a description of the class's function here \"\"\"\n",
    "        return(\"Topic clusters built from 2012 soda rules Tweets are vectorized using CountVectorizer to icludes 2grams in the vocab. \\\n",
    "                The 50 topic clusters are built from 2012 data soda rules, SVD applied, \\\n",
    "                kmeans for centroids. New tweets are labled by their nearness to centroids.Result returned provides score information.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = TopicModel()\n",
    "for thing in x.enrichment_value():\n",
    "    print thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
